{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWFZbDLEaOt2"
      },
      "source": [
        "# üè≠ Hackathon - D√©tection de D√©fauts Industriels avec IA\n",
        "\n",
        "## üìã Objectif\n",
        "Construire un syst√®me complet de d√©tection de d√©fauts sur des pi√®ces industrielles en utilisant:\n",
        "- **Machine Learning / Deep Learning** pour la classification\n",
        "- **CBIR** (Content-Based Image Retrieval) pour la recherche par similarit√©\n",
        "- **VLM** (Vision Language Model) pour la description automatique\n",
        "\n",
        "---\n",
        "\n",
        "## üóìÔ∏è Programme\n",
        "| Jour | Th√®me | Livrables |\n",
        "|------|-------|----------|\n",
        "| 1 | Fondamentaux | Pipeline ML/DL baseline |\n",
        "| 2 | Optimisation | Hyperparam√®tres + Interface Streamlit |\n",
        "| 3 | CBIR | Syst√®me de recherche par similarit√© |\n",
        "| 4 | VLM | Description automatique + Int√©gration finale |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è R√®gles du Hackathon\n",
        "- ‚úÖ Utiliser les ressources fournies\n",
        "- ‚úÖ Collaborer en √©quipe\n",
        "- ‚úÖ Documenter votre code\n",
        "- ‚ùå Ne pas copier le code final des autres √©quipes\n",
        "\n",
        "**Bonne chance! üöÄ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otk3ODZiaOt4"
      },
      "source": [
        "---\n",
        "# üì¶ SECTION 0: Configuration de l'Environnement\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCy2AwaRaOt5",
        "outputId": "69750247-bc87-488b-df4b-457ea4fe8f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (2.10.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (0.25.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (1.8.0)\n",
            "Requirement already satisfied: xgboost in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (3.2.0)\n",
            "Requirement already satisfied: lightgbm in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (4.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torch) (2026.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torchvision) (2.4.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from torchvision) (12.1.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from scikit-learn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (3.10.8)\n",
            "Requirement already satisfied: seaborn in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: pillow in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (12.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (2.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pyparsing>=3 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from seaborn) (3.0.0)\n",
            "Requirement already satisfied: tzdata in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\big p\\skema-hackathon\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: Invalid requirement: '#'\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Installation des d√©pendances (d√©commenter si n√©cessaire)\n",
        "!pip install torch torchvision scikit-learn xgboost lightgbm\n",
        "!pip install matplotlib seaborn pillow\n",
        "!pip install transformers accelerate  # Pour VLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IaP--zaOt7",
        "outputId": "426d642a-d2cd-4d16-e418-cf31d8aaa6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0+cpu\n",
            "CUDA available: False\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports de base\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6z6iwVFGaOt8"
      },
      "outputs": [],
      "source": [
        "# Configuration des chemins - √Ä MODIFIER selon votre structure\n",
        "DATA_DIR = Path(\"data\")\n",
        "TRAIN_DIR = DATA_DIR / \"train\"\n",
        "TEST_DIR = DATA_DIR / \"test\"\n",
        "MODELS_DIR = Path(\"./models\")\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Classes\n",
        "CLASSES = ['ok_front', 'def_front']\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Param√®tres images\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXtsJQ8iaOt9",
        "outputId": "f6e79d8f-b8ae-498b-ec13-94d1a9d86e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train exists: True\n",
            "Test exists: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Train exists:\", TRAIN_DIR.exists())\n",
        "print(\"Test exists:\", TEST_DIR.exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9El2q69JaOt_"
      },
      "source": [
        "---\n",
        "# üìä SECTION 1: Exploration des Donn√©es (EDA)\n",
        "---\n",
        "\n",
        "## üí° Objectif\n",
        "Comprendre la distribution de vos donn√©es avant de construire le mod√®le.\n",
        "\n",
        "## üéØ TODO pour votre √©quipe:\n",
        "1. Compter le nombre d'images par classe\n",
        "2. Visualiser quelques exemples\n",
        "3. V√©rifier la taille et qualit√© des images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hyA01HDaOt_",
        "outputId": "dc4ff226-1cb0-44a2-98b2-b426b59b4009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribution Train: {'defective': 2398, 'non_defective': 2871}\n",
            "Distribution Test: {'defective': 148, 'non_defective': 262}\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compl√©ter cette fonction pour compter les images\n",
        "\n",
        "import os\n",
        "def count_images(directory):\n",
        "    \"\"\"\n",
        "    Compte le nombre d'images par classe dans un r√©pertoire.\n",
        "\n",
        "    Args:\n",
        "        directory: Chemin vers le r√©pertoire (train ou test)\n",
        "\n",
        "    Returns:\n",
        "        dict: {'def_front': count, 'ok_front': count}\n",
        "    \"\"\"\n",
        "    counts = {}\n",
        "\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "\n",
        "        # V√©rifie que c'est bien un dossier\n",
        "        if os.path.isdir(class_path):\n",
        "            image_count = 0\n",
        "\n",
        "            for file in os.listdir(class_path):\n",
        "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    image_count += 1\n",
        "\n",
        "            counts[class_name] = image_count\n",
        "\n",
        "    return counts\n",
        "\n",
        "# Test\n",
        "print(\"Distribution Train:\", count_images(TRAIN_DIR))\n",
        "print(\"Distribution Test:\", count_images(TEST_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "c4iYDmPyaOuA",
        "outputId": "add2a943-5de0-4485-887b-b296d60a8a88"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] Le chemin d‚Äôacc√®s sp√©cifi√© est introuvable: 'data\\\\train\\\\ok_front'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m     plt.tight_layout()\n\u001b[32m     32\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mvisualize_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mvisualize_samples\u001b[39m\u001b[34m(directory, n_samples)\u001b[39m\n\u001b[32m     12\u001b[39m class_dir = directory / class_name\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m image_files = \u001b[43m[\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.jpeg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.png\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#2\u001b[39;00m\n\u001b[32m     21\u001b[39m selected_images = random.sample(image_files, \u001b[38;5;28mmin\u001b[39m(n_samples, \u001b[38;5;28mlen\u001b[39m(image_files)))\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     12\u001b[39m class_dir = directory / class_name\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m image_files = \u001b[43m[\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_dir\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.jpg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.jpeg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.png\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#2\u001b[39;00m\n\u001b[32m     21\u001b[39m selected_images = random.sample(image_files, \u001b[38;5;28mmin\u001b[39m(n_samples, \u001b[38;5;28mlen\u001b[39m(image_files)))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python311\\Lib\\pathlib.py:931\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    928\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m    932\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] Le chemin d‚Äôacc√®s sp√©cifi√© est introuvable: 'data\\\\train\\\\ok_front'"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Visualiser des exemples d'images\n",
        "\n",
        "def visualize_samples(directory, n_samples=4):\n",
        "    \"\"\"\n",
        "    Affiche des exemples d'images pour chaque classe.\n",
        "\n",
        "    Hint: Utilisez matplotlib avec subplot\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
        "\n",
        "    for i, class_name in enumerate(CLASSES):\n",
        "        class_dir = directory / class_name\n",
        "\n",
        "        #1\n",
        "        image_files = [\n",
        "            f for f in class_dir.iterdir()\n",
        "            if f.suffix.lower() in ['.jpg', '.jpeg', '.png']\n",
        "        ]\n",
        "\n",
        "        #2\n",
        "        selected_images = random.sample(image_files, min(n_samples, len(image_files)))\n",
        "\n",
        "        #3\n",
        "        for j, img_path in enumerate(selected_images):\n",
        "            img = Image.open(img_path)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].set_title(class_name)\n",
        "            axes[i, j].axis(\"off\")\n",
        "            pass\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(TRAIN_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Amr88oqaOuB"
      },
      "source": [
        "### ‚ùì Questions de r√©flexion:\n",
        "1. Vos classes sont-elles √©quilibr√©es?\n",
        "\n",
        "- Oui, les classes sont relativement √©quilibr√©es\n",
        "\n",
        "2. Quelles caract√©ristiques visuelles distinguent les d√©fauts?\n",
        "\n",
        "- On voit des t√¢ches sur les disques, de petits creux.\n",
        "\n",
        "3. Y a-t-il des variations de luminosit√©/angle dans les images?\n",
        "\n",
        "- Oui, on voit que les reflets de lumi√®re sont diff√©rents, les angles de prise de vue aussi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkhYPMJbaOuB"
      },
      "source": [
        "---\n",
        "# üñºÔ∏è SECTION 2: Pr√©traitement des Images\n",
        "---\n",
        "\n",
        "## üí° Concepts cl√©s\n",
        "\n",
        "### Normalisation ImageNet\n",
        "Les mod√®les pr√©-entra√Æn√©s (VGG, ResNet) attendent des images normalis√©es:\n",
        "- **Mean**: [0.485, 0.456, 0.406]\n",
        "- **Std**: [0.229, 0.224, 0.225]\n",
        "\n",
        "### Data Augmentation\n",
        "Techniques pour augmenter artificiellement le dataset:\n",
        "- Rotation, flip horizontal\n",
        "- Variation de luminosit√©\n",
        "- Crop al√©atoire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVz8DbhPaOuC",
        "outputId": "6dcdfccc-cffc-4944-d185-f499c6f2e7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Transforms d√©finis\n"
          ]
        }
      ],
      "source": [
        "# Transforms pour l'entra√Ænement (avec augmentation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Transforms pour test/inf√©rence (PAS d'augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Transforms d√©finis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKg6qPa9aOuC",
        "outputId": "aa283b31-4964-420e-fb2f-c9d017564a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre d'images: 5269\n"
          ]
        }
      ],
      "source": [
        "# TODO: Cr√©er un Dataset personnalis√©\n",
        "class DefectDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset pour les images de d√©fauts.\n",
        "\n",
        "    Structure attendue:\n",
        "    data_dir/\n",
        "    ‚îú‚îÄ‚îÄ defective/\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ *.jpg, *.png\n",
        "    ‚îî‚îÄ‚îÄ non_defective/\n",
        "        ‚îî‚îÄ‚îÄ *.jpg, *.png\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []  # Liste de (chemin_image, label)\n",
        "\n",
        "        for class_name in CLASSES:\n",
        "            class_dir = self.data_dir / class_name\n",
        "\n",
        "            # D√©finition du label\n",
        "            if class_name == \"ok_front\":\n",
        "                label = 0\n",
        "            elif class_name == \"def_front\":\n",
        "                label = 1\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Parcourir les images\n",
        "            for img_path in class_dir.iterdir():\n",
        "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                    self.samples.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        # Charger l'image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Appliquer les transformations si elles existent\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Test\n",
        "train_dataset = DefectDataset(TRAIN_DIR, transform=train_transform)\n",
        "print(f\"Nombre d'images: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRpkcg4saOuD"
      },
      "source": [
        "---\n",
        "# üß† SECTION 3: Extraction de Caract√©ristiques avec CNN\n",
        "---\n",
        "\n",
        "## üí° Concept: Transfer Learning\n",
        "\n",
        "Utiliser un CNN pr√©-entra√Æn√© sur ImageNet comme **extracteur de features**:\n",
        "1. Charger le mod√®le (VGG16, ResNet50, etc.)\n",
        "2. Retirer la derni√®re couche (classification)\n",
        "3. Passer l'image ‚Üí Obtenir un vecteur de features\n",
        "\n",
        "### Dimensions des features par mod√®le:\n",
        "| Mod√®le | Dimension |\n",
        "|--------|----------|\n",
        "| VGG16 | 4096 |\n",
        "| ResNet50 | 2048 |\n",
        "| DenseNet121 | 1024 |\n",
        "| MobileNetV2 | 1280 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2EH4KL6aOuD",
        "outputId": "af363f7d-c892-4be8-df07-2d95358bfdaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape des features: torch.Size([1, 2048])\n"
          ]
        }
      ],
      "source": [
        "# Exemple: Feature Extractor avec ResNet50\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extrait les features d'une image en utilisant un CNN pr√©-entra√Æn√©.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='resnet50'):\n",
        "        super().__init__()\n",
        "\n",
        "        if model_name == 'resnet50':\n",
        "            base_model = models.resnet50(pretrained=True)\n",
        "            # Retirer la derni√®re couche FC\n",
        "            self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
        "            self.output_dim = 2048\n",
        "\n",
        "        elif model_name == 'vgg16':\n",
        "            base_model = models.vgg16(pretrained=True)\n",
        "            self.features = base_model.features\n",
        "            self.avgpool = base_model.avgpool\n",
        "            # Utiliser seulement les features, pas le classifier\n",
        "            self.output_dim = 512 * 7 * 7  # 25088\n",
        "\n",
        "        # TODO: Ajouter d'autres mod√®les (DenseNet, MobileNet)\n",
        "\n",
        "        # Geler les poids (pas de fine-tuning)\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "extractor = FeatureExtractor('resnet50').to(DEVICE)\n",
        "extractor.eval()\n",
        "\n",
        "# Test avec une image al√©atoire\n",
        "dummy_img = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
        "features = extractor(dummy_img)\n",
        "print(f\"Shape des features: {features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for class_name in CLASSES:\n",
        "            class_dir = Path(data_dir) / class_name\n",
        "            label = 1 if class_name == \"def_front\" else 0\n",
        "\n",
        "            for img_path in class_dir.iterdir():\n",
        "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                    self.samples.append((str(img_path), label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image, label, img_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSDy6id0aOuE",
        "outputId": "5b76b189-9e81-48ae-912b-92ec92e2443a"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# ‚Üê Add this guard!\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     X_train, y_train, paths_train = \u001b[43mextract_all_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mextract_all_features\u001b[39m\u001b[34m(data_dir, extractor, transform, classes, batch_size, num_workers)\u001b[39m\n\u001b[32m     45\u001b[39m paths_list = []\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mImageDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     25\u001b[39m img_path, label = \u001b[38;5;28mself\u001b[39m.samples[idx]\n\u001b[32m     26\u001b[39m image = Image.open(img_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label, img_path\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/skema-hackathon/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:167\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    166\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    170\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
            "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "import torch.multiprocessing as mp\n",
        "mp.set_start_method('fork', force=True)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform, classes):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        for class_name in classes:\n",
        "            class_dir = Path(data_dir) / class_name\n",
        "            label = 1 if class_name == \"def_front\" else 0\n",
        "            for img_path in class_dir.iterdir():\n",
        "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
        "                    self.samples.append((str(img_path), label))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        return image, label, img_path\n",
        "\n",
        "def extract_all_features(data_dir, extractor, transform, classes, batch_size=32, num_workers=4):\n",
        "    dataset = ImageDataset(data_dir, transform, classes)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    device = next(extractor.parameters()).device\n",
        "    extractor.eval()\n",
        "    \n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    paths_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels, paths in dataloader:\n",
        "            images = images.to(device)\n",
        "            features = extractor(images)\n",
        "            features = features.cpu().numpy()\n",
        "            \n",
        "            features_list.append(features)\n",
        "            labels_list.append(labels.numpy())\n",
        "            paths_list.extend(paths)\n",
        "    \n",
        "    features_array = np.concatenate(features_list, axis=0)\n",
        "    labels_array = np.concatenate(labels_list, axis=0)\n",
        "    \n",
        "    return features_array, labels_array, paths_list\n",
        "\n",
        "# ‚Üê Add this guard!\n",
        "if __name__ == '__main__':\n",
        "    X_train, y_train, paths_train = extract_all_features(\n",
        "        TRAIN_DIR,\n",
        "        extractor,\n",
        "        test_transform,\n",
        "        CLASSES,\n",
        "        batch_size=64,\n",
        "        num_workers=0\n",
        "    )\n",
        "    print(f\"Features train: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy version: 2.4.2\n",
            "PyTorch version: 2.2.2\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch can access NumPy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "\n",
        "# print(f\"NumPy version: {np.__version__}\")\n",
        "# import torch\n",
        "# print(f\"PyTorch version: {torch.__version__}\")\n",
        "# print(f\"PyTorch can access NumPy: {torch.from_numpy(np.array([1,2,3]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9MJAWbaOuE"
      },
      "source": [
        "---\n",
        "# üß† SECTION 3b: Entra√Ænement CNN Baseline (Deep Learning)\n",
        "---\n",
        "\n",
        "## üí° Objectif\n",
        "Entra√Æner un CNN from scratch pour comparer avec l'approche hybride (features pr√©-entra√Æn√©es + shallow classifiers).\n",
        "\n",
        "### Architecture sugg√©r√©e:\n",
        "```\n",
        "Input (224√ó224√ó3)\n",
        "    ‚Üì\n",
        "Conv2D(32) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv2D(64) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv2D(128) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Conv2D(256) ‚Üí BatchNorm ‚Üí ReLU ‚Üí MaxPool\n",
        "    ‚Üì\n",
        "Global Average Pooling\n",
        "    ‚Üì\n",
        "FC(256) ‚Üí ReLU ‚Üí Dropout\n",
        "    ‚Üì\n",
        "FC(2) ‚Üí Softmax\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCA04GdPaOuF",
        "outputId": "024593c0-1476-4a4b-c3e1-3bd5e961658f"
      },
      "outputs": [],
      "source": [
        "# TODO: D√©finir le CNN Baseline\n",
        "class BaselineCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN personnalis√© pour la classification de d√©fauts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: D√©finir les couches convolutionnelles\n",
        "        # Hint: nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "        # Hint: nn.BatchNorm2d(num_features)\n",
        "        # Hint: nn.MaxPool2d(kernel_size)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 3 ‚Üí 32\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 2: 32 ‚Üí 64\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 3: 64 ‚Üí 128\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 4: 128 ‚Üí 256\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "             nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "model = BaselineCNN().to(DEVICE)\n",
        "dummy = torch.randn(1, 3, 224, 224).to(DEVICE)\n",
        "output = model(dummy)\n",
        "print(f\"Output shape: {output.shape}\")  # Devrait √™tre (1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIRnOI93Do-k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-BcF_OYaOuF"
      },
      "outputs": [],
      "source": [
        "def train_cnn(model, train_loader, val_loader, epochs=20, lr=0.0001):\n",
        "    \"\"\"\n",
        "    Entra√Æne le CNN avec early stopping.\n",
        "    Args:\n",
        "        model: Le CNN √† entra√Æner\n",
        "        train_loader: DataLoader d'entra√Ænement\n",
        "        val_loader: DataLoader de validation\n",
        "        epochs: Nombre d'√©poques maximum\n",
        "        lr: Learning rate\n",
        "    Returns:\n",
        "        model: Le mod√®le entra√Æn√©\n",
        "        history: Dict avec les m√©triques par √©poque\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=1e-4  # L2 regularization to prevent overfitting\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None  # Initialize to None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --------------------\n",
        "        # TRAIN\n",
        "        # --------------------\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # --------------------\n",
        "        # VALIDATION\n",
        "        # --------------------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = correct / len(val_loader.dataset)\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Sauvegarde historique\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # --------------------\n",
        "        # EARLY STOPPING\n",
        "        # --------------------\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()  # Save best model\n",
        "            print(f\"  ‚Üí New best model (val_loss: {val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"  ‚Üí No improvement ({patience_counter}/10)\")\n",
        "\n",
        "        if patience_counter >= 10:  # Increased patience from 5 to 10\n",
        "            print(\"Early stopping d√©clench√©\")\n",
        "            break\n",
        "\n",
        "    # Load best model before returning\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"\\nLoaded best model with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKSefLivaOuG"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Transformations pour l'entra√Ænement (augmentation + normalisation)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Transformations pour la validation/test (juste resize + tensor)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Cr√©er les DataLoaders\n",
        "train_dataset = DefectDataset(TRAIN_DIR, transform=train_transform)\n",
        "val_dataset = DefectDataset(TEST_DIR, transform=val_transform)\n",
        "\n",
        "# r√©duire la taille des batchs si run en local, sinon ca risque de consommer trop de m√©moire\n",
        "# particuli√®rement si le PC utilise un CPU ou un petit GPU\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ugZzfCaOuG",
        "outputId": "c7221c79-9aa4-4017-de67-a89ba0e2abef"
      },
      "outputs": [],
      "source": [
        "# Entra√Ænement\n",
        "cnn_model = BaselineCNN().to(DEVICE)\n",
        "cnn_model, history = train_cnn(cnn_model, train_loader, val_loader, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzQOPI-YIITO"
      },
      "outputs": [],
      "source": [
        "model = BaselineCNN().to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"baseline_cnn.pth\"))\n",
        "model.eval()  # Toujours mettre en mode √©valuation si tu fais des pr√©dictions\n",
        "torch.save(cnn_model.state_dict(), \"baseline_cnn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "LXcwto2DaOuH",
        "outputId": "a639ca97-ad9b-4846-a499-88291d0cbb9a"
      },
      "outputs": [],
      "source": [
        "# TODO: Visualiser les courbes d'apprentissage\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Affiche les courbes de loss et accuracy.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(history['train_loss'], label='Train Loss')\n",
        "    ax1.plot(history['val_loss'], label='Val Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.set_title('Training & Validation Loss')\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(history['val_acc'], label='Val Accuracy', color='green')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwKzR3oIaOuI"
      },
      "source": [
        "### üí° Optimisation du CNN (si n√©cessaire)\n",
        "\n",
        "Si les performances ne sont pas satisfaisantes:\n",
        "1. **Data Augmentation** plus agressive\n",
        "2. **Learning Rate Scheduler** (ReduceLROnPlateau)\n",
        "3. **Regularization** (Dropout, Weight Decay)\n",
        "4. **Architecture** (plus de couches, plus de filtres)\n",
        "5. **Transfer Learning** (fine-tuning d'un mod√®le pr√©-entra√Æn√©)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI9UZId_aOuI"
      },
      "outputs": [],
      "source": [
        "# TODO: Fine-tuning d'un mod√®le pr√©-entra√Æn√© (alternative au CNN from scratch)\n",
        "def create_finetuned_model(model_name='resnet50', num_classes=2, freeze_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Cr√©e un mod√®le pr√©-entra√Æn√© pour fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        model_name: 'resnet50', 'vgg16', etc.\n",
        "        num_classes: Nombre de classes (2 pour notre cas)\n",
        "        freeze_ratio: Proportion des couches √† geler (0.8 = 80% gel√©es)\n",
        "    \"\"\"\n",
        "    if model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        # Remplacer la derni√®re couche\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "        # Geler les premi√®res couches\n",
        "        layers = list(model.children())\n",
        "        freeze_until = int(len(layers) * freeze_ratio)\n",
        "        for i, layer in enumerate(layers[:freeze_until]):\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    # TODO: Ajouter VGG16, DenseNet si besoin\n",
        "\n",
        "    return model\n",
        "\n",
        "# model_ft = create_finetuned_model('resnet50').to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROSSHxyVaOuI"
      },
      "source": [
        "---\n",
        "# üìä SECTION 3c: Comparaison CNN vs Shallow Methods\n",
        "---\n",
        "\n",
        "## üéØ Objectif\n",
        "Comparer les performances de:\n",
        "1. **CNN Baseline** (entra√Æn√© from scratch)\n",
        "2. **CNN Fine-tun√©** (transfer learning)\n",
        "3. **Features pr√©-entra√Æn√©es + Shallow Classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx0t-Gz4aOuJ"
      },
      "outputs": [],
      "source": [
        "# TODO: Cr√©er un tableau de comparaison\n",
        "def compare_models(models_results):\n",
        "    \"\"\"\n",
        "    Compare les performances de plusieurs mod√®les.\n",
        "\n",
        "    Args:\n",
        "        models_results: Dict {'model_name': {'accuracy': ..., 'recall': ..., 'f1': ...}}\n",
        "\n",
        "    Returns:\n",
        "        DataFrame avec la comparaison\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(models_results).T\n",
        "    df = df.sort_values('f1', ascending=False)\n",
        "\n",
        "    # Afficher avec style\n",
        "    styled = df.style.background_gradient(cmap='Greens', subset=['accuracy', 'recall', 'f1'])\n",
        "\n",
        "    return styled\n",
        "\n",
        "# Exemple d'utilisation:\n",
        "# results = {\n",
        "#     'CNN Baseline': {'accuracy': 0.85, 'precision': 0.83, 'recall': 0.80, 'f1': 0.82},\n",
        "#     'ResNet50 + SVM': {'accuracy': 0.90, 'precision': 0.89, 'recall': 0.88, 'f1': 0.89},\n",
        "#     'ResNet50 + XGBoost': {'accuracy': 0.92, 'precision': 0.91, 'recall': 0.90, 'f1': 0.91},\n",
        "#     'VGG16 + RandomForest': {'accuracy': 0.88, 'precision': 0.86, 'recall': 0.85, 'f1': 0.86},\n",
        "# }\n",
        "# compare_models(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQRK8kTpaOuJ"
      },
      "outputs": [],
      "source": [
        "# Visualisation graphique de la comparaison\n",
        "def plot_model_comparison(results):\n",
        "    \"\"\"\n",
        "    Graphique de comparaison des mod√®les.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results).T.reset_index()\n",
        "    df = df.rename(columns={'index': 'Model'})\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    x = np.arange(len(df))\n",
        "    width = 0.2\n",
        "\n",
        "    ax.bar(x - width, df['accuracy'], width, label='Accuracy', color='#2E86AB')\n",
        "    ax.bar(x, df['recall'], width, label='Recall', color='#F18F01')\n",
        "    ax.bar(x + width, df['f1'], width, label='F1-Score', color='#4CAF50')\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Comparaison des Mod√®les')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='Objectif 90%')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# plot_model_comparison(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnA_-9W6aOuJ"
      },
      "source": [
        "---\n",
        "# üó≥Ô∏è SECTION 3d: Ensemble - Vote Majoritaire\n",
        "---\n",
        "\n",
        "## üí° Concept\n",
        "\n",
        "Combiner les pr√©dictions de plusieurs mod√®les:\n",
        "- Si la majorit√© pr√©dit \"D√©faut\" ‚Üí Pr√©diction finale = \"D√©faut\"\n",
        "- Confiance = nombre de votes pour la classe gagnante / total des mod√®les\n",
        "\n",
        "### Avantages:\n",
        "- Plus robuste qu'un seul mod√®le\n",
        "- R√©duit le risque de faux n√©gatifs\n",
        "- Capture diff√©rentes \"perspectives\" sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRurjro5aOuK"
      },
      "outputs": [],
      "source": [
        "class EnsemblePredictor:\n",
        "    \"\"\"\n",
        "    Pr√©dicteur ensemble avec vote majoritaire.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.models = {}  # {'model_name': (model, feature_extractor, scaler)}\n",
        "        self.cnn_models = {}  # CNN models qui pr√©disent directement\n",
        "\n",
        "    def add_shallow_model(self, name, model, feature_extractor, scaler):\n",
        "        \"\"\"\n",
        "        Ajoute un mod√®le shallow (SVM, XGBoost, etc.)\n",
        "        \"\"\"\n",
        "        self.models[name] = {\n",
        "            'model': model,\n",
        "            'extractor': feature_extractor,\n",
        "            'scaler': scaler\n",
        "        }\n",
        "\n",
        "    def add_cnn_model(self, name, model):\n",
        "        \"\"\"\n",
        "        Ajoute un mod√®le CNN (pr√©dit directement sur l'image)\n",
        "        \"\"\"\n",
        "        self.cnn_models[name] = model\n",
        "\n",
        "    def predict(self, image_tensor):\n",
        "        \"\"\"\n",
        "        Pr√©dit avec vote majoritaire.\n",
        "\n",
        "        Args:\n",
        "            image_tensor: Tensor de l'image (1, 3, 224, 224)\n",
        "\n",
        "        Returns:\n",
        "            dict: {\n",
        "                'prediction': 0 ou 1,\n",
        "                'confidence': float,\n",
        "                'votes': {'defective': n, 'non_defective': m},\n",
        "                'model_predictions': {'model_name': pred, ...}\n",
        "            }\n",
        "        \"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        # TODO: Pr√©dictions des CNN\n",
        "        for name, model in self.cnn_models.items():\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = model(image_tensor.to(DEVICE))\n",
        "                pred = output.argmax(dim=1).item()\n",
        "                predictions[name] = pred\n",
        "\n",
        "        # TODO: Pr√©dictions des shallow models\n",
        "        for name, components in self.models.items():\n",
        "            extractor = components['extractor']\n",
        "            scaler = components['scaler']\n",
        "            model = components['model']\n",
        "\n",
        "            # Extraire features\n",
        "            extractor.eval()\n",
        "            with torch.no_grad():\n",
        "                features = extractor(image_tensor.to(DEVICE))\n",
        "                features = features.cpu().numpy().flatten().reshape(1, -1)\n",
        "\n",
        "            # Normaliser\n",
        "            features_scaled = scaler.transform(features)\n",
        "\n",
        "            # Pr√©dire\n",
        "            pred = model.predict(features_scaled)[0]\n",
        "            predictions[name] = pred\n",
        "\n",
        "        # Vote majoritaire\n",
        "        votes = list(predictions.values())\n",
        "        defective_votes = sum(votes)\n",
        "        non_defective_votes = len(votes) - defective_votes\n",
        "\n",
        "        final_pred = 1 if defective_votes > non_defective_votes else 0\n",
        "        confidence = max(defective_votes, non_defective_votes) / len(votes)\n",
        "\n",
        "        return {\n",
        "            'prediction': final_pred,\n",
        "            'class_name': 'Defective' if final_pred == 1 else 'Non-Defective',\n",
        "            'confidence': confidence,\n",
        "            'votes': {\n",
        "                'defective': defective_votes,\n",
        "                'non_defective': non_defective_votes\n",
        "            },\n",
        "            'model_predictions': predictions\n",
        "        }\n",
        "\n",
        "# Exemple d'utilisation:\n",
        "# ensemble = EnsemblePredictor()\n",
        "# ensemble.add_cnn_model('CNN_Baseline', cnn_model)\n",
        "# ensemble.add_shallow_model('ResNet50_SVM', svm_model, resnet_extractor, scaler)\n",
        "# result = ensemble.predict(image_tensor)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw8T4LReaOuK"
      },
      "source": [
        "---\n",
        "# üéØ SECTION 4: Entra√Ænement des Classifiers\n",
        "---\n",
        "\n",
        "## üí° Approche Hybride\n",
        "Deep Features (CNN) + Shallow Classifiers (ML)\n",
        "\n",
        "### Classifiers √† tester:\n",
        "1. **SVM** - Support Vector Machine\n",
        "2. **Random Forest** - Ensemble d'arbres\n",
        "3. **XGBoost** - Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6__l5YZOaOuK"
      },
      "outputs": [],
      "source": [
        "# Normalisation des features (IMPORTANT!)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TODO: Normaliser vos features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdEexs3KaOuK"
      },
      "outputs": [],
      "source": [
        "# TODO: Entra√Æner un SVM\n",
        "def train_svm(X_train, y_train, X_val, y_val):\n",
        "        \n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "    \n",
        "    svm = SVC(probability=True, random_state=42)\n",
        "    grid = GridSearchCV(svm, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=0)\n",
        "    grid.fit(X_train, y_train)\n",
        "    \n",
        "    best_model = grid.best_estimator_\n",
        "    \n",
        "    y_pred = best_model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    \n",
        "    print(f\"   Best params: {grid.best_params_}\")\n",
        "    print(f\"   Val Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_random_forest(X_train, y_train, X_val, y_val):\n",
        "    \n",
        "    print(\"üîß Training Random Forest...\")\n",
        "    \n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    \n",
        "    print(f\"   Val Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_xgboost(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train XGBoost classifier.\"\"\"\n",
        "    \n",
        "    if not HAS_XGBOOST:\n",
        "        return None\n",
        "    \n",
        "    print(\"üîß Training XGBoost...\")\n",
        "    \n",
        "    model = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    \n",
        "    print(f\"   Val Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yRuCOzqaOuK"
      },
      "source": [
        "---\n",
        "# üìè SECTION 5: √âvaluation du Mod√®le\n",
        "---\n",
        "\n",
        "## üí° M√©triques importantes\n",
        "\n",
        "Dans le contexte industriel:\n",
        "- **Recall** est CRITIQUE (ne pas manquer de d√©fauts)\n",
        "- **F1-Score** pour la balance globale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN9rQawTaOuK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    √âvalue un mod√®le et affiche les m√©triques.\n",
        "    \"\"\"\n",
        "    # Pr√©dictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # M√©triques\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"R√âSULTATS D'√âVALUATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}  ‚¨ÖÔ∏è IMPORTANT!\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Matrice de confusion\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Non-D√©faut', 'D√©faut'],\n",
        "                yticklabels=['Non-D√©faut', 'D√©faut'])\n",
        "    plt.xlabel('Pr√©dit')\n",
        "    plt.ylabel('R√©el')\n",
        "    plt.title('Matrice de Confusion')\n",
        "    plt.show()\n",
        "\n",
        "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# Usage:\n",
        "results = evaluate_model(svm_model, X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ‚öôÔ∏è SECTION 6: Optimisation des Hyperparam√®tres (Jour 2)\n",
        "---\n",
        "\n",
        "## üí° Grid Search\n",
        "- Recherche exhaustive des meilleurs hyperparam√®tres.\n",
        "- Grid Search pour SVM, RandomForest, XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Grid Search pour SVM\n",
        "def optimize_svm(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trouve les meilleurs hyperparam√®tres pour SVM.\n",
        "    \n",
        "    Hint: GridSearchCV avec cv=5 (5-fold cross-validation)\n",
        "    \"\"\"\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "    \n",
        "    # VOTRE CODE ICI\n",
        "    # Hint: GridSearchCV(SVC(), param_grid, cv=5, scoring='f1')\n",
        "    \n",
        "    svm = SVC(probability=True, random_state=42)\n",
        "    grid = GridSearchCV(svm, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=0)\n",
        "    grid.fit(X_train, y_train)\n",
        "    \n",
        "    best_model = grid.best_estimator_\n",
        "    best_params = grid.best_params_\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = best_model.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    \n",
        "    print(f\"   Best params: {best_params}\")\n",
        "    print(f\"   Val Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    return best_model, best_params\n",
        "\n",
        "best_svm, best_params = optimize_svm(X_train_scaled, y_train)\n",
        "print(f\"Meilleurs param√®tres: {best_params}\")\n",
        "\n",
        "\n",
        "\n",
        "def train_svm(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train SVM with grid search.\"\"\"\n",
        "    \n",
        "    print(\"üîß Training SVM...\")\n",
        "    \n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['rbf', 'linear'],\n",
        "        'gamma': ['scale', 'auto']\n",
        "    }\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üåê Interface Streamlit (Jour 2-4)\n",
        "---\n",
        "\n",
        "## üí° Structure de l'application\n",
        "\n",
        "```python\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "\n",
        "st.title(\"üîç D√©tection de D√©fauts\")\n",
        "\n",
        "# Upload\n",
        "uploaded = st.file_uploader(\"Upload image\", type=['jpg', 'png'])\n",
        "\n",
        "if uploaded:\n",
        "    image = Image.open(uploaded)\n",
        "    st.image(image, caption=\"Image upload√©e\")\n",
        "    \n",
        "    if st.button(\"Analyser\"):\n",
        "        # 1. Pr√©diction\n",
        "        prediction, confidence = model.predict(image)\n",
        "        \n",
        "        # 2. Afficher r√©sultat\n",
        "        if prediction == 1:\n",
        "            st.error(f\"‚ö†Ô∏è D√âFAUT D√âTECT√â - Confiance: {confidence:.1%}\")\n",
        "        else:\n",
        "            st.success(f\"‚úÖ OK - Confiance: {confidence:.1%}\")\n",
        "        \n",
        "        \n",
        "```\n",
        "\n",
        "### Lancer l'application:\n",
        "```bash\n",
        "streamlit run app.py\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# üíæ Sauvegarde des Mod√®les\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "def save_pipeline(models_dir, classifier, scaler, cbir_data):\n",
        "    \"\"\"\n",
        "    Sauvegarde tous les composants n√©cessaires.\n",
        "    \"\"\"\n",
        "    models_dir = Path(models_dir)\n",
        "    models_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Classifier\n",
        "    joblib.dump(classifier, models_dir / 'classifier.pkl')\n",
        "    \n",
        "    # Scaler\n",
        "    joblib.dump(scaler, models_dir / 'scaler.pkl')\n",
        "    \n",
        "    # CBIR data\n",
        "    with open(models_dir / 'cbir_signatures.pkl', 'wb') as f:\n",
        "        pickle.dump(cbir_data, f)\n",
        "    \n",
        "    print(f\"‚úÖ Mod√®les sauvegard√©s dans {models_dir}\")\n",
        "\n",
        "# Usage:\n",
        "save_pipeline(\n",
        "    MODELS_DIR,\n",
        "    classifier=best_svm,\n",
        "    scaler=scaler,\n",
        "    cbir_data={'features': X_train, 'labels': y_train, 'paths': paths_train}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ‚úÖ CHECKLIST FINALE\n",
        "---\n",
        "\n",
        "## Jour 1\n",
        "- [X] EDA compl√®te\n",
        "- [X] Dataset et DataLoader fonctionnels\n",
        "- [X] Feature extraction avec CNN\n",
        "- [X] Baseline classifier entra√Æn√©\n",
        "- [X] M√©triques √©valu√©es\n",
        "\n",
        "## Jour 2\n",
        "- [X] Grid Search impl√©ment√©\n",
        "- [X] Meilleurs hyperparam√®tres trouv√©s\n",
        "- [X] Interface Streamlit de base\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
